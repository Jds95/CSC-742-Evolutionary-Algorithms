\documentclass{article}
\usepackage[utf8]{inputenc}

\title{An Evolutionary Computing Approach Toward the Lemmatization of Textual Data}
\author{Jesse Simpson, Amber Gillenwaters, Rafail Islam}
\date{December 2019}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Introduction}

The need for an increased understanding of natural language processing is universal in scope and potential utility. Businesses examine linguistic data to gain insights in meeting the needs of customers. Models of language classification contribute to the design of assistive technologies and teaching methods for individuals that may experience difficulties in reading, writing, visual or auditory comprehension, or in speech production. Evidence suggests that human communication shares characteristics in terms of neural processing regardless of language. Grammatical and semantic properties of words are an essential aspect of all linguistic models. However, many traditional approaches result in statistical models that are overfitted or too highly specified, resulting in a lack of applicability across language or domain. On the other hand, linguistic models aiming for generalization are often overly broad, also resulting low practical functionality \citep{nivre2015towards}. Recent increases in data accessibility, computational power, as well as innovative strategies in textual data manipulation has led to exponential growth and advancement in this topic area. This has contributed to increased demand for methodological development to improve upon existing theory and approaches. 

Though common in practice, when working with large datasets, repetitive manual adjustment is inefficient and unlikely to result in optimal results. Fine-tuning hyperparameters on various machine learning algorithms in textual data is particularly challenging, as the theoretical bases for natural language processing are still under debate and not fully developed. The application of evolutionary programming toward the language classification problem domain in order to optimize machine learning algorithms is a suitable target. As this project is exploratory, the eventual experimental design will consider and may incorporate multiple parameters associated with word classification and word discrimination. Schmidt \citep{schmidt2019performance} demonstrates the capacity of differential evolution in hypertuning parameters allowing for a multimodal design. That said, focus on tuning a single procedural step may be preferable, allowing for more trials and greater detail.

This project has the potential for supporting or challenging the linear discrimination approach proposed by \citep{baayen2019discriminative}. For instance, \citep{baayen2019discriminative} acknowledge that the application of a na√Øve Bayes classification model or neural network comparison may serve to supplement their findings. Alternatively, Lee, Lim, and Ahn \citep{lee2019automotive} suggest that graph-directed models may improve on tree-based models. \cite{gleim2019practitioner} assessed lemmatization comparing fine-grained parts of speech as opposed to coarser groups, a parameter with potential for optimization using genetic algorithms. \citep{baayen2019discriminative} and \citep{buchanan_dedeyne_montefinese_2019} used the tree-tagger library developed by Schmid \citep{schmid1994probabilistic, schmid1999improvements}. Though tree-tagger is common in research, the library is deserving of review as it is more well-suited to smaller datasets. Adjusting decision trees by applying genetic algorithms using a random forest model may increase performance or contribute to improvements in word classification, producing stop-words, spell-checking, parts of speech, and semantic meaning most closely approximating natural language processing. 

The data used are from \citep{buchanan_dedeyne_montefinese_2019} consisting of over 16,000 concept-feature responses. In a concept-feature task, respondents are provided with a cue consisting of a single word to which they are asked to provide a typed response. These responses are analyzed and nominally classified. Words were classified as part of speech, including adjectives, nouns, and verbs. These were further classified as either concrete or abstract terms. Concrete terms are more easily visualized and sensed, while abstract terms require more higher-order thinking. The process identifies stop words, generates lemmas, groups of words with properties in common. Relationship encoding is based on distance. Because the data has been processed and labeled, the set is well-suited for experimentation.  
The population will consist of lemmas produced, with variation in hyperparameters and value ranges within the decision tree or alternative classification method. The dependent variables may include computational speed, cost, and either alignment or improvement upon preexisting models (such as a reduction in dimensionality). In order to implement this project, sci-kit learn \citep{pedregosa2011scikit}, NumPy \citep{oliphant2006guide}, and pandas \citep{mckinney2010data} will be the primary existing packages utilized, with variation in logical application. These libraries are open-source and well-established in literature. In order to represent the problem in code, we will use sci-kit learn to implement the various machine learning algorithms. Tree-tagger is an open-source decision-tree library commonly used for lemmatization including annotation and part of speech classification. A critical assessment of parameters used in tree-tagger is a suitable target for improvement. After we have selected the exact algorithms, we will use their specific hyperparameters as our population. This will be represented by a list of numbers and strings, where each index in the list will represent its corresponding hyperparameter.  

As a means to evaluate the child population for comparison with an existing created population, a simple evaluation with low overhead will be developed using an accuracy metric in which we measure how well each machine learning algorithm performs in terms of accurately classifying a sample with the specified hyperparameters. Accuracy, feature importance, practical application, and computational complexity will be considered in the fitness of the results. 

\section{Methods}

\section{Results}

\section{Discussion}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
